{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b593271-0d64-4d35-8505-aa415299c2f8",
   "metadata": {
    "id": "UkVKimgLvoRp"
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">\n",
    "      <h1>Lighthouse Labs</h1>\n",
    "      <h2>W7D5 - Pipelines and Model Persistence</h2>\n",
    "      <strong>Instructor:</strong> Socorro E. Dominguez-Vidana\n",
    "    </td>\n",
    "    <td style=\"text-align: right;\">\n",
    "      <img src=\"img/lhl.jpeg\" alt=\"LHL\" width=\"200\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5e3ee-c709-45a4-a838-45ad976752a1",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sedv8808/LHL_Lectures/main?labpath=W7D5%2FW7D5_Pipelines_and_Model_Persistence.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee873691-b789-4b86-9b7d-38a5d60b6c99",
   "metadata": {},
   "source": [
    "## Overview - Pipelines\n",
    "- [] Motivation and example\n",
    "- [] Feature unions\n",
    "- [] Column transformers\n",
    "- [] Visualizing pipelines\n",
    "- [] Hyperparameter tuning with pipelines\n",
    "- [] Custom class in a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f57426-2564-4da0-8d1c-79f347aadb39",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\"><img src=\"img/mario_luigi.jpg\" alt=\"Mario and Luigi fixing pipes\" width=\"600\">\n",
    "<td> Mario and Luigi have been hired to get water flowing smoothly through a set of pipes. Each part of the job has to be done in the right order, and they use their plumbing pipeline to keep it organized.\n",
    "    </td>\n",
    "</tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca606b-3357-480f-ad5c-29b5bb85b25d",
   "metadata": {},
   "source": [
    "1.\tClear the Pipes\n",
    "Luigi goes first, clearing out any clogs and making sure the pipes are clean. In a **ML pipeline**, this is like **cleaning the data**\n",
    "\n",
    "2.\tCheck the Pipe Sizes\n",
    "Next, Mario measures and standardizes the pipes to ensure they're compatible. This step would be similar to **scaling** or **normalizing** the data. \n",
    "\n",
    "3.\tInstall Filters\n",
    "To keep the water clean, Luigi adds filters at specific points in the pipeline. Similarly, we may add steps to transform data or select only the features we need, so only relevant information goes through.\n",
    "\n",
    "4.\tTest the Water Flow\n",
    "Before they finish, Mario and Luigi test the water flow to make sure everything works. In ML, this step is like **training** and **testing** a model guaranteeing that our setup can handle real-world data.\n",
    "\n",
    "5.\tFinal Check and Save the Setup\n",
    "Finally, they document how they set up the pipes so anyone can understand it if they need repairs later. In ML, we can save the pipeline model with `joblib` or `pickle` so we can use it again without setting it up from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e059ebf-9706-4080-8a2a-a597a757332e",
   "metadata": {},
   "source": [
    "#### Why Use a Pipeline?\n",
    "- Reuse steps easily for new data.\n",
    "- Test everything consistently and know the exact order, so there are no surprises in model performance.\n",
    "\n",
    "In simple terms, an ML pipeline is our plumbing plan for handling data from start to finish, making sure every step flows correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac81304-27da-46f0-bce1-decf06fdf091",
   "metadata": {},
   "source": [
    "#### Using pipelines\n",
    "\n",
    "Besides being a plumber, Mario took up another role: **Doctor Mario**! \n",
    "\n",
    "The famous plumber-turned-physician is on a new mission: tackling diabetes! In his clinic, he got access to the [diabetes dataset](https://www.kaggle.com/datasets/mathchi/diabetes-data-set) that holds information on patients' `age`, `blood pressure`, `BMI`, and more. With this data, Dr. Mario can identify risk factors, predict who might be at risk, and create better treatment plans.\n",
    "\n",
    "Just like fixing a tricky pipe, understanding diabetes requires the right tools and steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e8e31-63a0-45da-930e-8ce2ac9b64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c551de1-c682-4d40-b114-f24a192131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25624a0e-526a-40b6-b93f-7eafbc8e384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8ad11-7121-46f6-aa8d-237dd5f3a7ae",
   "metadata": {},
   "source": [
    "##### Without a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10e3e3-b6e0-432c-a250-2d950ddfe33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='class')\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Test portion\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "y_pred = model.predict(X_test_pca)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Test set accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6721ecd-c0b7-40b4-b06e-ef890854d97d",
   "metadata": {},
   "source": [
    "There are several inconvenient things about this:\n",
    "1. We have a lot of ugly code. We keep calling `.fit()` and `.transform()` on different objects, and\n",
    "we keep having to rename transformed variables so as not to cause confusions later in our notebook.\n",
    "2. Our preprocessing and modeling code is distributed and therefore error-prone. If we try running our model \n",
    "somewhere else and forget to copy over a step (e.g. we don't apply StandardScaler to the test set), \n",
    "then our model will not work as expected.\n",
    "3. We can only use convenient Sklearn functions/classes such as `GridSearchCV()` on the *model class* (e.g. LogisticRegression). What if we want to try different numbers of components, or different scaling methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ef7d6-bde5-4ee4-9506-f7d0b4c97353",
   "metadata": {},
   "source": [
    "#### The solution: Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747751a-bd89-46b1-a81f-b3797aff87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[('scaling', StandardScaler()),\n",
    "                           ('pca', PCA(n_components=3)),\n",
    "                           ('classifier', LogisticRegression())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Test set accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55010df9-070a-433f-9597-d067bf4307a1",
   "metadata": {},
   "source": [
    "Notice how much cleaner this code is. The composite model created using `Pipeline`\n",
    "can be used just like any other Sklearn model you have learned, which means that it\n",
    "can also be passed to functions like `cross_val_score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ec775-fa42-4ef9-8b84-f109b0969c58",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature unions\n",
    "\n",
    "`Pipeline` lets us specify a sequence of steps that will be executed in one after the other (i.e. in `series`). But it wouldn't make sense if Mario always had to wait for Luigi to finish a job. Sometimes, Mario and Luigi can each work on different parts of a job at the same time, then combining their results at the end.\n",
    "\n",
    "In ML, sometimes we want to apply different transformations to our data in parallel and then combine the results. We might want to scale numerical features (like `age` or `pregnancies`) while applying one-hot encoding to categorical features (like `gender` or `city`). Instead of doing this in separate steps, `FeatureUnion` lets us do both at once and then joins them together.\n",
    "`FeatureUnion`'s can be composed with `Pipeline`'s however much we want.\n",
    "\n",
    "![](img/series_and_parallel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480d20-0412-42c4-a510-8b191397b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "feature_union = FeatureUnion([('pca', PCA(n_components=3)), \n",
    "                              ('select_best', SelectKBest(k=6))])\n",
    "\n",
    "pipeline = Pipeline(steps=[('scaling', StandardScaler()),\n",
    "                           ('features', feature_union),\n",
    "                           ('classifier', LogisticRegression())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Test set accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efa9f4-e1b7-4d5f-8b8b-fda2f40279bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Column transformers\n",
    "\n",
    "`FeatureUnion` doesn’t automatically \"split\" the data between **transformers**. It applies each **transformer** to the full dataset. For different transformations applied to specific parts of the data, we use `ColumnTransformer`, which allows you to specify which columns should go to which transformation.\n",
    "\n",
    "For instance, for a dataset with both numerical and categorical features,\n",
    "we may want to do something like the following:\n",
    "1. For *numeric* columns:\n",
    "    1. Impute missing values with the *mean*\n",
    "    2. Standard scale the values\n",
    "2. For *categorical* columns:\n",
    "    1. Impute missing values with the *mode*\n",
    "    2. One-hot-encode the categories\n",
    "3. Fit a model to the resulting features\n",
    "\n",
    "`ColumnTransformer` has a very similar syntax to that of a `FeatureUnion` or `Pipeline`, except that we must also specify the *column names* that each transform applies to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96cd7e-b548-47f9-91aa-a4fe09113e50",
   "metadata": {},
   "source": [
    "Doctor Mario is tackling heart disease now! With patient data on `age`, `cholesterol`, `heart rate`, and more, he aims to identify key risk factors to help his patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ae879-3ea0-4e6f-95f8-b1106eae80ba",
   "metadata": {},
   "source": [
    "| **Feature**    | **Description**                                                        |\n",
    "|----------------|------------------------------------------------------------------------|\n",
    "| `age`          | Age of the patient in years                                            |\n",
    "| `sex`          | Sex of the patient (1 = male, 0 = female)                              |\n",
    "| `cp`           | Chest pain type (0: typical angina, 1: atypical angina, 2: non-anginal pain, 3: asymptomatic) |\n",
    "| `trestbps`     | Resting blood pressure in mm Hg                                       |\n",
    "| `chol`         | Serum cholesterol in mg/dl                                            |\n",
    "| `fbs`          | Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)                  |\n",
    "| `restecg`      | Resting electrocardiographic results (0: normal, 1: ST-T wave abnormality, 2: probable left ventricular hypertrophy) |\n",
    "| `thalach`      | Maximum heart rate achieved                                           |\n",
    "| `exang`        | Exercise-induced angina (1 = yes, 0 = no)                             |\n",
    "| `oldpeak`      | ST depression induced by exercise relative to rest                    |\n",
    "| `slope`        | Slope of the peak exercise ST segment (0: upsloping, 1: flat, 2: downsloping) |\n",
    "| `ca`           | Number of major vessels (0-3) colored by fluoroscopy                  |\n",
    "| `thal`         | Thalassemia (3 = normal, 6 = fixed defect, 7 = reversible defect)     |\n",
    "| `target`       | Diagnosis of heart disease (1 = disease, 0 = no disease)              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbda6a5-d6d4-42a2-8652-54e7fabaa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/heart_disease.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d2cf8-1a5b-4f3f-93bc-770046647675",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns='target'), df['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984b509-178f-411c-a212-2825f7c9787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "categorical_features = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "numeric_transform = StandardScaler()\n",
    "categorical_transform = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer([('numeric', numeric_transform, numerical_features), \n",
    "                                  ('categorical', categorical_transform, categorical_features)])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression())\n",
    "])\n",
    "                     \n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e176685-e2b7-44bf-aa71-fafa77aeaa67",
   "metadata": {},
   "source": [
    "What is the `ColumnTransformer` actually doing? We can get a better sense by looking at our data before and after\n",
    "the transform it applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478632c-1a88-4d41-a345-92fb7c1c8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd10690-8fee-44a8-bd87-a401ed8dce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648e855-bc12-4ba2-9a64-34a2a0b90454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data\n",
    "X_preprocessed = preprocessor.transform(X)\n",
    "X_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304ebb6-cec1-4fc7-b99f-183464bdb26e",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizing pipelines\n",
    "Another advantage of having these pipelines is that we can quickly visualize complex workflows used in our\n",
    "modeling as HTML, which can be helpful for debugging purposes or presentations.\n",
    "\n",
    "<sub>*Note: I highly recommend you use this in your own presentations as a substitute for (or in addition to) code.*</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe50048-33d6-47bb-b447-08d40e06b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML representation in a jupyter context\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4a1b8-2bcb-4a90-ad1c-1e1a04063f03",
   "metadata": {},
   "source": [
    "Note that you can also click on the individual parts in the diagram (e.g. `StandardScaler`) to see their arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a91fd-d184-42b1-bc78-d8d5f45b912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, save the HTML to a file\n",
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('img/model_pipeline.html', 'w') as f:  \n",
    "    f.write(estimator_html_repr(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af7517-8542-48b6-befd-5956589b9709",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter tuning with pipelines\n",
    "<table>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\"><img src=\"img/mario.png\" alt=\"Mario fixing pipes\" width=\"3000\">\n",
    "<td> \n",
    "\n",
    "Normally, if we want to tune hyperparameters using something like `GridSearchCV`, we need to pass it:\n",
    "1. A model object.\n",
    "2. A dictionary of (parameter name, list of values to try) pairs.\n",
    "\n",
    "When not using pipelines, we can only tune hyperparameters for a single model (the one we specify as the\n",
    "model in `GridSearchCV`. As we've seen, however, we can create composite models using `Pipeline`. We can\n",
    "then pass this composite model to `GridSearchCV` and tune hyperparameters for multiple components at once.\n",
    "    </td>\n",
    "</tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff3c94-47bc-4ced-a87f-c04d8dfc572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 3, 4],\n",
    "    'classifier__C': [0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['liblinear', 'lbfgs']}\n",
    "\n",
    "# GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Cross-Validation Score: {grid_search.best_score_}')\n",
    "\n",
    "# Evaluate the test set with the best model found\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "acc = best_pipeline.score(X_test, y_test)\n",
    "print(f'Test set accuracy with best parameters: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6afebe-f699-4190-afc2-ecf1f40116bf",
   "metadata": {},
   "source": [
    "In addition to trying out different hyperparameters for a given step in the pipeline, you can also try different classes altogether. For instance, what if we wanted to try both Logistic Regression and SVM for the \"classifier\" step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72fb12-55f2-4421-a54e-ff89f143e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('classifier', LogisticRegression())  # Placeholder for classifier\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'pca__n_components': [2, 3, 4],\n",
    "        'classifier': [LogisticRegression()],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    {\n",
    "        'pca__n_components': [2, 3, 4],\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    {\n",
    "        'pca__n_components': [2, 3, 4],\n",
    "        'classifier': [RidgeClassifier()],\n",
    "        'classifier__alpha': [0.1, 0.01, 1.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5e04f-d0ad-4c5f-b5d3-034113676ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e217a-2cbf-4085-a195-6bdaae5136f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312cade-d116-4bb5-9989-850d8cc8205f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Persistence\n",
    "\n",
    "After hours of fixing pipes, Mario and Luigi don't want to start from scratch if they have to revisit the job. In ML, pipelines can take a long time to `.fit`, and you will not want to run the whole notebook every single time - we want to save the model so we can use it again without retraining.\n",
    "\n",
    "This process is called **model persistence**. Just like Mario and Luigi might record their work notes, we use `pickle` or `joblib` to save models. This lets us load them back up later ready to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d0622-8faf-494b-8f56-1b4032365981",
   "metadata": {},
   "source": [
    "---\n",
    "Serialization is the process of converting a program entity into a stream of bytes that can be saved as a file.\n",
    "Serialization: \n",
    "- Avoids redundant training. Models can take a long time to train and data can take a long time to load/process\n",
    "- allows us to deploy the model into an application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1e3ea-1a65-42d3-9ec2-5f90e6e3e4a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Pickle\n",
    "- Pickling is the process where a Python object is converted into a byte stream (usually not human readable).\n",
    "- Unpickling is the reverse operation, where a byte stream is converted back into a working Python object.\n",
    "- Pickling is the simplest way to store the object from a coding perspective.\n",
    "- The Python Pickle module is an object-oriented way of storing objects.\n",
    "    - It can store *any* Python object, not just `Sklearn` models.\n",
    "    \n",
    "#### Features\n",
    "- Store/load dictionaries and lists.\n",
    "- Store/load the attributes of arbitrary data types (i.e. classes)\n",
    "- Do this recursively, so that if your object has attributes that are\n",
    "classes themselves, it can be saved just as easily\n",
    "\n",
    "#### Limitations\n",
    "- Does not save the *code* of an object — only its attribute values.\n",
    "- Cannot save file handles or connection sockets.\n",
    "- Pickle is **version-dependent**. For example, if you saved a model with a certain version of `Sklearn` then try to load it with a different one (e.g. you updated), there may be issues.\n",
    "    - Another motivation for using virtual environments, which can be containerized.\n",
    "\n",
    "#### Saving procedure\n",
    "```python\n",
    "import pickle        # Built-in python module\n",
    "\n",
    "# Create some object and manipulate it in some way (e.g. train the model)\n",
    "myobj = SomeClass(...)\n",
    "myobj = myobj.some_method(...)\n",
    "\n",
    "# Save to a file using Pickle\n",
    "with open('myfile.pickle', 'wb') as file_handle:\n",
    "    pickle.dump(myobj, file_handle)\n",
    "```\n",
    "\n",
    "#### Loading procedure\n",
    "```python\n",
    "import pickle        # Built-in python module\n",
    "\n",
    "# Load from a file using Pickle\n",
    "with open('myfile.pickle', 'rb') as file_handle:\n",
    "    myobj = pickle.load(file_handle)    # myobj will be an instance of SomeClass\n",
    "```\n",
    "\n",
    "#### Methods\n",
    "The pickle module provides four different methods:\n",
    "- dump() − The dump() method serializes to an open file (file-like object).\n",
    "- dumps() − Serializes to a string.\n",
    "- load() − Deserializes from an open-like object.\n",
    "- loads() − Deserializes from a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d6759-9857-4bde-a85e-a6ec336b882b",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5d925-b5a8-4a2f-bf24-d31d88a38635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Save the model\n",
    "with open('saved_models/pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Load the model\n",
    "with open('saved_models/pipeline.pickle', 'rb') as f:\n",
    "    pipeline_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73f540-894e-427b-b477-192ce28319b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc63d5-dcd8-4b85-9b9b-f63d87bef3d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Joblib\n",
    "Joblib is an alternative serialization module to Pickle. However, starting with Python 3.8, Pickle is actually better than Joblib for saving `numpy` arrays.  \n",
    "<sub>  If you have Python >=3.8, just use Pickle. [Source](https://stackoverflow.com/a/12617603).*</sub>\n",
    "    \n",
    "#### Saving procedure\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Create some object and manipulate it in some way (e.g. train the model)\n",
    "myobj = SomeClass(...)\n",
    "myobj = myobj.some_method(...)\n",
    "\n",
    "# Save to a file using Joblib\n",
    "joblib.dump(myobj, file_path)\n",
    "```\n",
    "\n",
    "#### Loading procedure\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load from a file using Joblib\n",
    "myobj = joblib.load(file_path)    # myobj will be an instance of SomeClass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd656b0-fa1a-4ef2-95e7-3cb27468ce17",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccde44-10c6-45f8-bd0b-49f75404013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(grid_search, 'saved_models/pipeline.can')\n",
    "\n",
    "# Load the model\n",
    "pipeline_loaded = joblib.load('saved_models/pipeline.can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a7769-d391-48c5-b059-baa2dacb7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_loaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
