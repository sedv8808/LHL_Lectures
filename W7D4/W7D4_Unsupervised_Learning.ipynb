{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkVKimgLvoRp"
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">\n",
    "      <h1>Lighthouse Labs</h1>\n",
    "      <h2>W7D4 - Unsupervised Learning</h2>\n",
    "      <strong>Instructor:</strong> Socorro E. Dominguez-Vidana\n",
    "    </td>\n",
    "    <td style=\"text-align: right;\">\n",
    "      <img src=\"img/lhl.jpeg\" alt=\"LHL\" width=\"200\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sedv8808/LHL_Lectures/main?labpath=W7D4%2FW7D4_Unsupervised_Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JukDw-rvoRt"
   },
   "source": [
    "Overview:\n",
    "- [] Different types of learnings in machine learning.\n",
    "- [] Unsupervised learning use cases\n",
    "- [] Different types of clustering\n",
    "- [] Evaluation of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning:\n",
    "- Learning from data without explicit labels, where the algorithm finds hidden **patterns** or intrinsic **structures**.\n",
    "- Market segmentation, anomaly detection, and exploratory data analysis.\n",
    "\n",
    "![Unsupervised Learning](https://cdn-images-1.medium.com/max/1440/1*YUl_BcqFPgX49sSb5yrk3A.jpeg)\n",
    "\n",
    "[Author unknown. (n.d.). Data Science vs. Machine Learning Medium. ](https://cdn-images-1.medium.com/max/1440/1*YUl_BcqFPgX49sSb5yrk3A.jpeg)\n",
    "\n",
    "\n",
    "#### Reinforcement Learning:\n",
    "- Learning by interacting with an environment to maximize cumulative reward.\n",
    "- Game AI, robotics, optimization problems.\n",
    "\n",
    "![Reinforcement Learning](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NCOUSqJtdblFskOw-yHQsQ.png)\n",
    "\n",
    "[Rana, P. (2020, March 25). Reinforcement learning overview. Medium.](https://medium.com/@preeti.rana.ai/reinforcement-learning-overview-636ab135476e)\n",
    "\n",
    "\n",
    "#### Semi-supervised Learning:\n",
    "- Learning from a small amount of labeled data combined with a large amount of unlabeled data.\n",
    "- Text classification, image recognition.\n",
    "![Semi Supervised Learning](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mywd8wQNZLYUJmUyElmACA.jpeg)\n",
    "[Sharma, G. (2020, June 20). A gentle introduction to semi-supervised learning. Medium.](https://medium.com/@gayatri_sharma/a-gentle-introduction-to-semi-supervised-learning-7afa5539beea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customer Segmentation:\n",
    "    - Clustering algorithms group customers based on purchasing behavior, helping businesses identify different market segments.\n",
    "\n",
    "- Anomaly Detection:\n",
    "    - Identify unusual data points, often used for fraud detection, network security, and medical diagnosis.\n",
    "\n",
    "- Recommendation Systems:\n",
    "\t- Algorithms like K-means clustering can recommend products by grouping similar users or products based on features.\n",
    "\n",
    "- Dimensionality Reduction (**PCA**):\n",
    "    - Used for reducing the complexity of data, particularly when dealing with high-dimensional datasets, while preserving variance for tasks like visualization or preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering \n",
    "\n",
    "- Clustering is the task of **dividing** a dataset into **groups** (or **clusters**) such that the data points in the same group are more similar to each other than to those in other groups.\n",
    "\n",
    "**Clustering Algorithms:**\n",
    "- Partitioning Methods:\n",
    "    - `K-Means`: Divides the data into **K** clusters based on minimizing variance within clusters.\n",
    "- Hierarchical Methods:\n",
    "    - `Agglomerative Hierarchical Clustering`: A bottom-up approach where each observation starts in its own cluster, and clusters are merged iteratively.\n",
    "    - `Divisive Hierarchical Clustering`: A top-down approach where all data points start in one cluster and are split iteratively.\n",
    "- Density-Based Methods:\n",
    "    - `DBSCAN` (Density-Based Spatial Clustering of Applications with Noise): Clusters based on density, identifying areas of high data point concentration. Works well with irregularly shaped clusters and is robust to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Gremlin Risk Management Task Force” in Kingston Falls\n",
    "\n",
    "<img src=\"img/job_add.png\" alt=\"gremlins\" width=\"500\">\n",
    "\n",
    "Imagine that you are a new Data Scientist at Kingston Falls's <strong>Gremlin Risk Management Task Force</strong> and you have to develop a system that categorizes Gremlins based on their **behaviors**, **physical traits**, and **environmental exposure**.\n",
    "\n",
    "The goal is to manage the risks posed by different types of Gremlins, ranging from harmless *Mogwai* to the most dangerous and aggressive Gremlins. The town **lacks** labeled data about what makes a Gremlin *dangerous* or *manageable*.\n",
    "\n",
    "The office gives you the Gremlins dataset with the following columns:\n",
    "\n",
    "| Feature                 | Description                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------|\n",
    "| **Size (cm)**            | The height of the Gremlin or Mogwai in centimeters.                                         |\n",
    "| **Weight (kg)**          | The weight of the Gremlin or Mogwai in kilograms.                                           |\n",
    "| **Color_Intensity**      | A scale (0-100) representing the intensity of the creature's color (darker = more intense). |\n",
    "| **Aggressiveness**       | A scale (1-10) measuring how aggressive the Gremlin is (1 = docile, 10 = highly aggressive).|\n",
    "| **Intelligence**         | A scale (1-10) measuring the creature's intelligence.                                       |\n",
    "| **Number of Spikes**     | The number of spikes or physical protrusions on the Gremlin (0 for Mogwai).                 |\n",
    "| **Age (years)**          | The age of the Gremlin or Mogwai in years.                                                  |\n",
    "| **Moisture Exposure (hrs)** | The number of hours the creature has been exposed to moisture (triggers Gremlin transformation). |\n",
    "| **Fed_After_Midnight**   | Binary feature (0 = not fed after midnight, 1 = fed after midnight, which transforms Mogwai to Gremlins). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gremlins_df = pd.read_csv('data/gremlins.csv')\n",
    "gremlins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "- Builds a hierarchy of clusters, either agglomeratively (bottom-up) or divisively (top-down). We will focus on agglomerative clustering here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linked = linkage(gremlins_df, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** The y-axis in the dendrogram represents the **dissimilarity** or **distance** between clusters, with higher points indicating the merging of more dissimilar clusters. \n",
    "\n",
    "- A large gap at the top (`~250 mark`) suggests a strong division between the two main branches (orange and green), indicating **two** distinct groups.\n",
    "\n",
    "- Leaves at the bottom represent individual Gremlins, and as you move up, they merge into larger clusters based on similarity.\n",
    "\n",
    "- To choose the number of clusters, you can \"cut\" the dendrogram at a particular height. For example, cutting at around 100 on the `y-axis` yields three main clusters (one large orange group, one smaller green group, and an intermediate group - probably `Mogwai-like`, `Mischievous`, and `Highly Aggressive` Gremlins).\n",
    "\n",
    "- A lower cut (around 50) would result in smaller, more detailed clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hierarchichal Clustering StatsQuest](https://www.youtube.com/watch?v=7xHsRkOdVwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Clustering\n",
    "\n",
    "- Algorithm based on **Expectation Maximization**\n",
    "- `K` is a hyperparameter that stands for the number of clusters.\n",
    "- **Random** initial centroids\n",
    "- Data points assigned to clusters based on Euclidean (or other type of) distance.\n",
    "- Cluster centroids updated based on data points\n",
    "- Steps repeat until convergence\n",
    "\n",
    "##### Characteristics:\n",
    "- Simple to understand and interpret\n",
    "- Some convergence is guaranteed\n",
    "- Can work with very large datasets\n",
    "- Number of clusters needs to be determined\n",
    "- May converge on local optimum\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=5, n_init=10)\n",
    "kmeans_model.fit(gremlins_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gremlins_df['Kmeans_Cluster'] = kmeans_model.fit_predict(gremlins_df)\n",
    "gremlins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can I visualize the 5 clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot clusters using 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gremlins_df['Size (cm)'], gremlins_df['Weight (kg)'], c=gremlins_df['Kmeans_Cluster'], cmap='viridis')\n",
    "plt.xlabel('Size (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('K-Means Clustering of Gremlins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "This plot displays the `K-Means` clustering results for the Gremlins dataset, with the points colored according to their assigned cluster. \n",
    "\n",
    "Here, only **two features** —*Size (cm)* on the `x-axis` and *Weight (kg)* on the `y-axis`— are used for visualization, but the clustering model itself considered all features in the dataset to determine the clusters.\n",
    "\n",
    "1.\tClusters Representation:\n",
    "    - Each color represents a distinct cluster identified by K-Means. Points with similar characteristics (across all features, not just size and weight) are grouped into the same cluster.\n",
    "    - It appears that there are 4 distinct clusters based on color, meaning **K-Means** found **four** main groupings in the dataset.\n",
    "\n",
    "2.\tCluster Separation:\n",
    "    - The purple cluster (right side of the plot) includes Gremlins that are relatively larger and heavier. These could represent older or more transformed Gremlins.\n",
    "\t- The yellow cluster (left side of the plot) seems to include Gremlins that are generally smaller and lighter, possibly resembling more Mogwai-like characteristics.\n",
    "\t- The green and blue clusters occupy the central areas and show more moderate values for size and weight, indicating a mix of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do I know how many clusters `k` I should do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for k in range(1, 10):\n",
    "  model = KMeans(n_clusters=k, n_init=10)\n",
    "  model.fit(gremlins_df)\n",
    "  scores[k] = model.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=scores.keys(), y=scores.values())\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia (SSE)')\n",
    "plt.title('K-Means Elbow Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the **elbow method plot**, the *elbow* or *inflection point* appears around 3 clusters, which suggests that three is likely an ideal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat with only 3 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=3, n_init=10)\n",
    "kmeans_model.fit(gremlins_df)\n",
    "gremlins_df['Kmeans_Cluster'] = kmeans_model.fit_predict(gremlins_df)\n",
    "gremlins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(gremlins_df['Size (cm)'], gremlins_df['Weight (kg)'], c=gremlins_df['Kmeans_Cluster'], cmap='viridis')\n",
    "plt.xlabel('Size (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('K-Means Clustering of Gremlins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StatsQuest KMeans](https://www.youtube.com/watch?v=4b5d3muPQmA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Why does it look less clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "- `DBSCAN` is a density-based clustering algorithm that groups together points that are close to each other and marks outliers that are in low-density regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=1, min_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gremlins_df['DBSCAN_Cluster'] = dbscan.fit_predict(gremlins_df)\n",
    "gremlins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gremlins_df['Size (cm)'], gremlins_df['Weight (kg)'], c=gremlins_df['DBSCAN_Cluster'], cmap='plasma')\n",
    "plt.xlabel('Size (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('DBSCAN Clustering of Gremlins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation and Analysis of DBSCAN Result**\n",
    "In this plot, `DBSCAN` attempted (and failed) to identify clusters in the Gremlins dataset. All points were marked as `-1` indicating that they were all marked as outliers (noise).\n",
    "\n",
    "##### Why DBSCAN Failed?\n",
    "\n",
    "1.\t**Unscaled Features**:\n",
    "\t- `DBSCAN` as well as `K-Means` rely on distance measurements to define clusters. When features (e.g., `size` and `weight`) have different scales, the larger values can dominate the distance calculations, skewing the results.\n",
    "\n",
    "2.\tImpact of Distance-Based Approaches:\n",
    "\t- Distance-based algorithms **require** `scaled data` to compute meaningful distances between points.\n",
    "  \n",
    "[DBScan StatsQuest](https://www.youtube.com/watch?v=RDZUdRSDOok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "`StandardScaler` is a technique used to **normalize** the features in a dataset by removing the mean and scaling to unit variance. It transforms each feature to have a **mean** of `0` and a **standard deviation** of `1`, putting all features on the same scale.\n",
    "\n",
    "In the Gremlins dataset, features like `Size (cm)` and `Weight (kg)` have different ranges. For example:\n",
    "- Size might range from 15 to 45 cm.\n",
    "- Weight could range from 5 to 15 kg.\n",
    "\n",
    "Size has a larger numerical range than Weight. Without scaling, distance-based algorithms will give more importance to Size because it has a higher range, leading to interpret Gremlins with similar sizes as being closer, regardless of their weight differences.\n",
    "\n",
    "With `StandardScaler`, we make each feature equally important by standardizing their values. This allows distance-based algorithms to consider both Size and Weight fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(gremlins_df.drop(['Kmeans_Cluster', 'DBSCAN_Cluster'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=5, min_samples=2)\n",
    "gremlins_df['DBSCAN_Cluster_Scaled'] = dbscan.fit_predict(scaled_data)\n",
    "gremlins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gremlins_df['Size (cm)'], gremlins_df['Weight (kg)'], c=gremlins_df['DBSCAN_Cluster_Scaled'], cmap='plasma')\n",
    "plt.xlabel('Size (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('DBSCAN Clustering (Scaled) of Gremlins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `DBSCAN` works best with data that has distinct, dense regions separated by sparse areas. If the Gremlins dataset has more uniform or scattered points without clear dense groupings, DBSCAN will label most points as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA (Principal Component Analysis)\n",
    "\n",
    "`PCA` is a dimensionality reduction technique that projects data into fewer dimensions while retaining most of the variance.\n",
    "\n",
    "##### Uses:\n",
    "- **Unsupervised ML model**: `PCA` helps explore patterns in unlabeled data, capturing the directions of maximum variance.\n",
    "- **Preprocessing technique**: `PCA` reduces dimensionality and improves model efficiency, making it an **essential step** for high-dimensional datasets in both unsupervised and supervised learning contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Does `PCA` work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gremlins_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The `gremlins_df` has 150 rows and 12 columns. I cannot visualize the 12 dimensions (features).\n",
    "\n",
    "- I want a new matrix (`df`) with only 2 columns that represents the information in all the 12 columns.\n",
    "\n",
    "$$\n",
    "\\textbf{X} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &  &   & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    \\textbf{X}_{1}    & \\textbf{X}_{2}    & \\ldots & \\ldots & \\ldots & \\textbf{X}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &   &  & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{d columns (wider)}}\\\\\n",
    "\\textbf{Z} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} &         & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    {\\textbf{Z}}_{1}    & \\ldots & \\textbf{Z}_{k}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex}  &        & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{k columns (narrower)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine we only had 2 features: `size` and `weight`.\n",
    "\n",
    "And we plot them this way:  \n",
    "![img1](imgs/PCA01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our task is to project this data into a smaller dimension: a line.\n",
    "\n",
    "First, for all observations, we calculate the average measure for `Height` and then, the average measure for `Weight`\n",
    "\n",
    "![img1](imgs/PCA01a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's shift the data in such a manner that the center of the data, becomes the origin.\n",
    "\n",
    "![img1](imgs/PCA01b.png)\n",
    "\n",
    "** Data points are still related among themselves the same way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that the data is centered, let's `fit` a **random** line that captures most of our data points information.   \n",
    "**This line MUST pass through the origin.**\n",
    "\n",
    "![img1](imgs/PCA01c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `PCA` projects the data on the line.\n",
    "- `PCA` finds the line that maximizes the distances from the projected points to the origin.\n",
    "- This is the same as minimizing the distance between the line and the data observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. PCA will measure the distance from the origin to each projected observation. \n",
    "\n",
    "2. If we only had 5 observations, it would only have 5 distances:\n",
    "\n",
    "    $d_1 + d_2 + d_3 + d_4 + d_5$\n",
    "\n",
    "3. and then, squares them up:\n",
    "\n",
    "    ${d_1}^2 + {d_2}^2 + {d_3}^2 + {d_4}^2 + {d_5}^2 = SS(distances)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img1](imgs/PCA01d.png)\n",
    "\n",
    "We do this until we get the **largest** $SS(distances)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This new line is called **Principal Component 1 (PC1)**\n",
    "\n",
    "**SS(distances) for PC1** is called the **eigenvalue for PC1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say that our **PC1** has a slope of 0.5:\n",
    "\n",
    "- For every 2 unit increase in **height**, we increase 1 unit in **weight**.\n",
    "\n",
    "- Then for `PC1`, **height** is more important than **weight**.\n",
    "\n",
    "**Data is more spread out on the height axis**\n",
    "\n",
    "**PC1** ends up being a **Linear Combination** of:   \n",
    "> $PC1 = 2*Height + 1*Weight$\n",
    "\n",
    "When we make the vector have a measure of one, by normalizing it, we end up having the **eigenvector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](imgs/eigenvector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With 2 features, it is easy to find **PC2**, it has to be the line that also passes through the origin and that is ortogonal to **PC1**.  \n",
    "\n",
    "If we had 3 or more features, to find **PC2** we would have to repeat the process:  \n",
    "i)find the best fitting line that also:  \n",
    "ii) passes through the origin  \n",
    "iii) is perpendicular/ortogonal to **PC1**  \n",
    "  \n",
    "Then **PC3** would just have to:  \n",
    "a) pass through the origin  \n",
    "b) be perpendicular to **PC1** and **PC2**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](imgs/PCA02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our final plot, we rotate everything so that PC1 and PC2 are horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](imgs/PCA03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Variation\n",
    "\n",
    "We will aid ourselves with a `Scree Plot` to measure variation. \n",
    "\n",
    "$Variation(PC1) = \\frac{SS(distances_{PC1})}{n-1}$  \n",
    "$Variation(PC2) = \\frac{SS(distances_{PC2})}{n-1}$  \n",
    "...  \n",
    "$Variation(PCn) = \\frac{SS(distances_{PCn})}{n-1}$  \n",
    "\n",
    "![img](imgs/scree_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a **Scree Plot** you might determine that you only need the first 2 or 3 PCs rather than the complete set of PCs for a better model.\n",
    "\n",
    "Remember, the max number of PCs that you have:  \n",
    "a) number of features  \n",
    "b) number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA StatsQuest](https://www.youtube.com/watch?v=FgakZw6K1QQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's apply `PCA` to the Gremlins dataset for Visualization Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit(gremlins_df.drop(['Kmeans_Cluster', 'DBSCAN_Cluster', 'DBSCAN_Cluster_Scaled'], axis=1))\n",
    "pca_components = pca.transform(gremlins_df.drop(['Kmeans_Cluster', 'DBSCAN_Cluster', 'DBSCAN_Cluster_Scaled'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=gremlins_df['Kmeans_Cluster'], cmap='viridis')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Gremlins Dataset with K-Means Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_kmeans = silhouette_score(gremlins_df.drop(['Kmeans_Cluster',\n",
    "       'DBSCAN_Cluster', 'DBSCAN_Cluster_Scaled'], axis=1), gremlins_df['Kmeans_Cluster'])\n",
    "print(f'Silhouette Score for K-Means: {silhouette_kmeans}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `PCA` to the Gremlins dataset as a Preprocessing Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.fit_predict(pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "cmap = ListedColormap([\"purple\", \"teal\", \"yellow\"])\n",
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=kmeans_labels, cmap=cmap)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('KMeans Clustering on PCA-Reduced Data')\n",
    "\n",
    "stripe_img = mpimg.imread('img/Stripe.webp') \n",
    "gizmo_img = mpimg.imread('img/gizmo.png') \n",
    "brain_img = mpimg.imread('img/brain.jpeg')\n",
    "\n",
    "centroids = np.array([pca_components[kmeans_labels == i].mean(axis=0) for i in range(3)])\n",
    "\n",
    "def add_image(ax, img, x, y, zoom=0.2):\n",
    "    imagebox = OffsetImage(img, zoom=zoom)\n",
    "    ab = AnnotationBbox(imagebox, (x, y), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "ax = plt.gca()\n",
    "add_image(ax, gizmo_img, centroids[0, 0], centroids[0, 1], zoom=0.1)\n",
    "add_image(ax, stripe_img, centroids[1, 0], centroids[1, 1], zoom=0.1) \n",
    "add_image(ax, brain_img, centroids[2, 0], centroids[2, 1], zoom=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('img/Stripe.webp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does PCA Component 1 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_pc1 = pca.components_[0]  # Loadings for the first principal component\n",
    "feature_importance_pc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = gremlins_df.drop(columns=['Kmeans_Cluster', 'DBSCAN_Cluster', 'DBSCAN_Cluster_Scaled']).columns\n",
    "pc1_importance_df = pd.DataFrame({'Feature': features, 'PC1 Loading': feature_importance_pc1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_importance_df['Absolute Loading'] = pc1_importance_df['PC1 Loading'].abs()\n",
    "pc1_importance_df = pc1_importance_df.sort_values('Absolute Loading', ascending=False)\n",
    "pc1_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca = pca.fit(gremlins_df.drop(['Kmeans_Cluster', 'DBSCAN_Cluster', 'DBSCAN_Cluster_Scaled'], axis=1))\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot (Bar Chart)')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Feature Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Reducing the number of features in a dataset\n",
    "    - E.g., 1000 rows by 20 columns (features) to 1000 rows by 10 columns\n",
    "- Helps our machine learning algorithms perform better\n",
    "- Improves run-time of our algorithms\n",
    "- Storing and using less data (memory)\n",
    "- For visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why do dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For visualization: \n",
    "    - The human visual system only works in up to 3 dimensions.\n",
    "    - Our screens are really only 2D.\n",
    "- To improve the performance of our baseline model  \n",
    "\n",
    "**It may or may not work. You will not know until you try.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "The easiest way to reduce features is to keep the most important features and \"eliminating\" the others.\n",
    "\n",
    "The resulting feature set will still be interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Feature Selection Techniques: Filter and Wrapper Methods\n",
    "\n",
    "##### Filter methods\n",
    "\n",
    "- Measure relevance of feature by correlation with dependent variable (target).\n",
    "- If feature is correlated with target, keep. Otherwise, discard\n",
    "- Applied before training ML model\n",
    "- Advantages: \n",
    "    - Fast, no training involved\n",
    "- Disadvantages: \n",
    "    - Ignores feature combinations\n",
    "    - Keeps redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![imgs](imgs/Filter_Methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapper methods\n",
    "- Train ML model with different subsets of feature\n",
    "- If feature improves performance, add/keep it. Otherwise, ignore/remove it.\n",
    "- Applied during training ML model\n",
    "- Advantages:\n",
    "    - Evaluates features in context of others\n",
    "    - Performance-driven\n",
    "- Disadvantages:\n",
    "    - Slow, retrain model several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![imgs](imgs/Wrapper_Methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward selection wrapper method\n",
    "\n",
    "1. SelectedFeatures = [ ]\n",
    "2. Find F in (AllFeatures - SelectedFeatures) that, if added to SelectedFeatures, best improves model performance\n",
    "3. If adding F improved performance more than some threshold, permanently add it to SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward elimination wrapper method\n",
    "\n",
    "1. SelectedFeatures = AllFeatures\n",
    "2. Find F in SelectedFeatures that, if removed from SelectedFeatures, decreases model performance the least\n",
    "3. If removing F decreased performance less than some threshold, permanently remove it from SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "1. Decide $k$, the number of features to select. \n",
    "* Use a model (usually a linear model) to assign weights to features.\n",
    "    - The weights of important features have higher absolute value.\n",
    "* Rank the features based on the absolute value of weights.\n",
    "* Drop the least useful feature.\n",
    "* Try steps 2-4 again until desired number of features is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Wrapper Methods Tips\n",
    "- Look for implementations, `sklearn` has a `rfe` implementations, for example\n",
    "- It's not possible to tell which method will work better until you try\n",
    "- Different variable selection algorithms may give you a different answers\n",
    "- Different machine learning algorithms with the same variable selection method may give you given answers\n",
    "- Over this process, you'll find out what features tend to get eliminated and which features tend to be kept (hopefully)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
